{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5-2 데이터 연결 기초\n",
    "import pandas as pd \n",
    "\n",
    "df1 = pd.read_csv('c:\\\\work\\\\concat_1.csv')\n",
    "df2 = pd.read_csv('c:\\\\work\\\\concat_2.csv')\n",
    "df3 = pd.read_csv('c:\\\\work\\\\concat_3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D\n",
      "0   a0   b0   c0   d0\n",
      "1   a1   b1   c1   d1\n",
      "2   a2   b2   c2   d2\n",
      "3   a3   b3   c3   d3\n",
      "0   a4   b4   c4   d4\n",
      "1   a5   b5   c5   d5\n",
      "2   a6   b6   c6   d6\n",
      "3   a7   b7   c7   d7\n",
      "0   a8   b8   c8   d8\n",
      "1   a9   b9   c9   d9\n",
      "2  a10  b10  c10  d10\n",
      "3  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "#concat메서드는 데이터프레임을 연결할 때 위에서 \n",
    "#아래방향으로 연결한다. \n",
    "row_concat = pd.concat([df1,df2,df3])\n",
    "print(row_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    a3\n",
      "B    b3\n",
      "C    c3\n",
      "D    d3\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#연결한 데이터프레임에서 행 데이터를 추출해본다.\n",
    "#다음은 데이터프레임에서 네번째 행을 추출한 것이다.\n",
    "print(row_concat.iloc[3,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터프레임에 시리즈 연결하기\n",
    "#이번에는 데이터프레임에 시리즈를 추가한다. \n",
    "#먼저 리스트를 시리즈로 변환한다.\n",
    "new_row_series = pd.Series(['n1','n2','n3','n4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D    0\n",
      "0   a0   b0   c0   d0  NaN\n",
      "1   a1   b1   c1   d1  NaN\n",
      "2   a2   b2   c2   d2  NaN\n",
      "3   a3   b3   c3   d3  NaN\n",
      "0  NaN  NaN  NaN  NaN   n1\n",
      "1  NaN  NaN  NaN  NaN   n2\n",
      "2  NaN  NaN  NaN  NaN   n3\n",
      "3  NaN  NaN  NaN  NaN   n4\n"
     ]
    }
   ],
   "source": [
    "#concat메서드로 데이터프레임과 시리즈를 연결해 본다. \n",
    "#시리즈가 새로운 행으로 추가될 것 같죠? \n",
    "#하지만 행이 아니라 시리즈는 새로운 열로 추가한다. 그래서 NaN이라는 값도 많이 \n",
    "#생겼다. NaN은 누락값이다.\n",
    "print(pd.concat([df1, new_row_series]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  n1  n2  n3  n4\n"
     ]
    }
   ],
   "source": [
    "#행이 1개라도 반드시 데이터프레임이 담아 연결해야 한다. \n",
    "#시리즈는 행이 1개인 데이터프레임이라고 생각해도 된다. \n",
    "#다음은 1개의 행을 가지는 데이터프레임을 생성하여 df1에 연결한 것이다.\n",
    "new_row_df = pd.DataFrame([['n1','n2','n3','n4']],\n",
    "    columns=['A','B','C','D'])\n",
    "print(new_row_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "0  n1  n2  n3  n4\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([df1, new_row_df]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "0  n1  n2  n3  n4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee\\AppData\\Local\\Temp\\ipykernel_11636\\100693829.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  print(df1.append(new_row_df))\n"
     ]
    }
   ],
   "source": [
    "#concat메서드는 한번에 2개 이상의 데이터프레임을 연결할 수 있는 메서드이다.\n",
    "#만약 연결한 데이터프레임이 1개라면 append메서드를 사용해도 된다. \n",
    "print(df1.append(new_row_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "4  n1  n2  n3  n4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee\\AppData\\Local\\Temp\\ipykernel_11636\\2189997302.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  print(df1.append(data_dict, ignore_index=True))\n"
     ]
    }
   ],
   "source": [
    "#append메서드와 딕셔너리를 사용하면 더욱 간편하게 행을 연결할 수 있다. \n",
    "#이때 ignore_index를 True로 지정하면 인덱스를 다시 설정한다.\n",
    "data_dict = {'A':'n1', 'B':'n2', 'C':'n3', 'D':'n4'}\n",
    "print(df1.append(data_dict, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A    B    C    D\n",
      "0    a0   b0   c0   d0\n",
      "1    a1   b1   c1   d1\n",
      "2    a2   b2   c2   d2\n",
      "3    a3   b3   c3   d3\n",
      "4    a4   b4   c4   d4\n",
      "5    a5   b5   c5   d5\n",
      "6    a6   b6   c6   d6\n",
      "7    a7   b7   c7   d7\n",
      "8    a8   b8   c8   d8\n",
      "9    a9   b9   c9   d9\n",
      "10  a10  b10  c10  d10\n",
      "11  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "#다양한 방법으로 데이터 연결하기 \n",
    "#ignore_index인자 사용하기\n",
    "row_concat_i = pd.concat([df1,df2,df3], ignore_index=True)\n",
    "print(row_concat_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   A   B   C   D    A    B    C    D\n",
      "0  a0  b0  c0  d0  a4  b4  c4  d4   a8   b8   c8   d8\n",
      "1  a1  b1  c1  d1  a5  b5  c5  d5   a9   b9   c9   d9\n",
      "2  a2  b2  c2  d2  a6  b6  c6  d6  a10  b10  c10  d10\n",
      "3  a3  b3  c3  d3  a7  b7  c7  d7  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "#열 방향으로 데이터 연결하기 \n",
    "#만약 행 방향이 아니라 열 방향으로 데이터를 연결하려면 어떻게 해야 하나요?\n",
    "#concat메서드의 axis인자를 1로 지정하면 된다. \n",
    "#다음은 df1,df2,df3을 열방향으로 연결한 것이다. \n",
    "#axis의 기본값은 0이다. \n",
    "col_concat = pd.concat([df1,df2,df3], axis=1)\n",
    "print(col_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   A    A\n",
      "0  a0  a4   a8\n",
      "1  a1  a5   a9\n",
      "2  a2  a6  a10\n",
      "3  a3  a7  a11\n"
     ]
    }
   ],
   "source": [
    "#만약 같은 열 이름이 있는 데이터프레임에서 \n",
    "#열이름으로 데이터를 추출하면 해당 열 이름의 데이터를\n",
    "#모두 추출한다.\n",
    "print(col_concat['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   A   B   C   D    A    B    C    D new_col_list\n",
      "0  a0  b0  c0  d0  a4  b4  c4  d4   a8   b8   c8   d8           n1\n",
      "1  a1  b1  c1  d1  a5  b5  c5  d5   a9   b9   c9   d9           n2\n",
      "2  a2  b2  c2  d2  a6  b6  c6  d6  a10  b10  c10  d10           n3\n",
      "3  a3  b3  c3  d3  a7  b7  c7  d7  a11  b11  c11  d11           n4\n"
     ]
    }
   ],
   "source": [
    "#다음과 같이 입력하면 간편하게 새로운 열을 추가할 수도 있다.\n",
    "col_concat['new_col_list'] = ['n1','n2','n3','n4']\n",
    "print(col_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7    8    9    10   11\n",
      "0  a0  b0  c0  d0  a4  b4  c4  d4   a8   b8   c8   d8\n",
      "1  a1  b1  c1  d1  a5  b5  c5  d5   a9   b9   c9   d9\n",
      "2  a2  b2  c2  d2  a6  b6  c6  d6  a10  b10  c10  d10\n",
      "3  a3  b3  c3  d3  a7  b7  c7  d7  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "#위의 과정에서 데이터프레임의 열 이름을 유지한 채 연결했기 때문에\n",
    "#열 이름이 중복되었다. 다음은 ignore_index를 True로 지정하여\n",
    "#열 이름을 다시 지정한 것이다.\n",
    "print(pd.concat([df1,df2,df3], axis=1, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "    E   F   G   H\n",
      "0  a4  b4  c4  d4\n",
      "1  a5  b5  c5  d5\n",
      "2  a6  b6  c6  d6\n",
      "3  a7  b7  c7  d7\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "     A    C    F    H\n",
      "0   a8   b8   c8   d8\n",
      "1   a9   b9   c9   d9\n",
      "2  a10  b10  c10  d10\n",
      "3  a11  b11  c11  d11\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#공통열과 공통인덱스만 연결하기\n",
    "#만약 열 이름의 일부가 서로 다른 데이터프레임을 연결하면 어떻게 될까?\n",
    "#앞에서 사용한 df1,df2,df3의 열 이름을 다시 지정한다. \n",
    "df1.columns = ['A','B','C','D']\n",
    "df2.columns = ['E','F','G','H']\n",
    "df3.columns = ['A','C','F','H']\n",
    "print(df1)\n",
    "print(type(df1))\n",
    "\n",
    "print(df2)\n",
    "print(type(df2))\n",
    "\n",
    "print(df3)\n",
    "print(type(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D    E    F    G    H\n",
      "0   a0   b0   c0   d0  NaN  NaN  NaN  NaN\n",
      "1   a1   b1   c1   d1  NaN  NaN  NaN  NaN\n",
      "2   a2   b2   c2   d2  NaN  NaN  NaN  NaN\n",
      "3   a3   b3   c3   d3  NaN  NaN  NaN  NaN\n",
      "0  NaN  NaN  NaN  NaN   a4   b4   c4   d4\n",
      "1  NaN  NaN  NaN  NaN   a5   b5   c5   d5\n",
      "2  NaN  NaN  NaN  NaN   a6   b6   c6   d6\n",
      "3  NaN  NaN  NaN  NaN   a7   b7   c7   d7\n",
      "0   a8  NaN   b8  NaN  NaN   c8  NaN   d8\n",
      "1   a9  NaN   b9  NaN  NaN   c9  NaN   d9\n",
      "2  a10  NaN  b10  NaN  NaN  c10  NaN  d10\n",
      "3  a11  NaN  b11  NaN  NaN  c11  NaN  d11\n"
     ]
    }
   ],
   "source": [
    "#새롭게 열 이름을 부여한 데이터프레임 3개를 concat메셔드로 연결한다.\n",
    "#이렇게 하면 열 이름이 정렬되며 연결된다. \n",
    "#그리고 데이터프레임에 없는 열 이름의 데이터는 누락값으로 처리된다. \n",
    "#누락값 없이 데이터를 연결하는 방법은 없을까? \n",
    "row_concat = pd.concat([df1,df2,df3])\n",
    "print(row_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#데이터프레임의 공통 열만 골라 연결하면 누락값이 생기지 않는다. \n",
    "#공통 열만 골라서 연결하려면 join인자를 inner로 지정해야 한다. \n",
    "#아쉽게도 df1,df2,df3은 공통열이 없다. \n",
    "#따라서 세 데이터프레임의 공통 열을 연결한 결과값으로 Empty DataFrame\n",
    "#이 출력된다. \n",
    "print(pd.concat([df1,df2,df3], join='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    C\n",
      "0   a0   c0\n",
      "1   a1   c1\n",
      "2   a2   c2\n",
      "3   a3   c3\n",
      "0   a8   b8\n",
      "1   a9   b9\n",
      "2  a10  b10\n",
      "3  a11  b11\n"
     ]
    }
   ],
   "source": [
    "#df1, df3의 공통 열만 골라 연결해 본다.\n",
    "#그러면 공통 열인 A와 C만 연결된다. \n",
    "print(pd.concat([df1,df3], ignore_index=False, join='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "    E   F   G   H\n",
      "4  a4  b4  c4  d4\n",
      "5  a5  b5  c5  d5\n",
      "6  a6  b6  c6  d6\n",
      "7  a7  b7  c7  d7\n",
      "     A    C    F    H\n",
      "0   a8   b8   c8   d8\n",
      "2   a9   b9   c9   d9\n",
      "5  a10  b10  c10  d10\n",
      "7  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "#이번에는 데이터프레임을 행 방향으로 연결해 본다.\n",
    "#df1,df2,df3의 인덱스를 다시 지정해 본다.\n",
    "df1.index = [0,1,2,3]\n",
    "df2.index = [4,5,6,7]\n",
    "df3.index = [0,2,5,7]\n",
    "\n",
    "print(df1)\n",
    "print(df2)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D    E    F    G    H    A    C    F    H\n",
      "0   a0   b0   c0   d0  NaN  NaN  NaN  NaN   a8   b8   c8   d8\n",
      "1   a1   b1   c1   d1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "2   a2   b2   c2   d2  NaN  NaN  NaN  NaN   a9   b9   c9   d9\n",
      "3   a3   b3   c3   d3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "4  NaN  NaN  NaN  NaN   a4   b4   c4   d4  NaN  NaN  NaN  NaN\n",
      "5  NaN  NaN  NaN  NaN   a5   b5   c5   d5  a10  b10  c10  d10\n",
      "6  NaN  NaN  NaN  NaN   a6   b6   c6   d6  NaN  NaN  NaN  NaN\n",
      "7  NaN  NaN  NaN  NaN   a7   b7   c7   d7  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "#concat메서드로 df1,df2,df3을 열 방향으로 연결하면 앞의 과정과\n",
    "#비슷한 결과가 출력된다. \n",
    "col_concat = pd.concat([df1,df2,df3], axis=1)\n",
    "print(col_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   A   C   F   H\n",
      "0  a0  b0  c0  d0  a8  b8  c8  d8\n",
      "2  a2  b2  c2  d2  a9  b9  c9  d9\n"
     ]
    }
   ],
   "source": [
    "#앞의 과정과 비슷한 방법으로 df1, df3의 공통 행만 골라서 연결해 본다.\n",
    "#그러면 공통행인 0과 2만 출력된다.\n",
    "print(pd.concat([df1,df3], axis=1, join='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#외부 조인과 내부 조인\n",
    "#내부조인: 둘 이상의 데이터프레임에서 조건에 맞는 행을 연결하는 것\n",
    "#외부조인: 두 데이터프레임에서 어떤 데이터프레임을 기준으로 할 것인지에\n",
    "#따라 왼쪽 외부 조인(Left Outer Join)과 오른쪽 외부 조인(Right Outer Join)\n",
    "#완전 외부 조인(Full Outer Join)으로 나눌 수 있다. \n",
    "#왼쪽 외부 조인은 왼쪽 데이터프레임을 모두 포함하여 연결하는 것이고\n",
    "#오른쪽 외부 조인은 오른쪽 데이터프레임을 모두 포함하여 연결하는 것이다.\n",
    "#완전 외부 조인은 왼쪽과 오른쪽 데이터프레임을 모두 포함하여 연결한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ident   personal    family\n",
      "0      dyer    William      Dyer\n",
      "1        pb      Frank   Pabodie\n",
      "2      lake   Anderson      Lake\n",
      "3       roe  Valentina   Roerich\n",
      "4  danforth      Frank  Danforth\n",
      "    name    lat    long\n",
      "0   DR-1 -49.85 -128.57\n",
      "1   DR-3 -47.15 -126.72\n",
      "2  MSK-4 -48.87 -123.40\n",
      "   ident   site       dated\n",
      "0    619   DR-1  1927-02-08\n",
      "1    622   DR-1  1927-02-10\n",
      "2    734   DR-3  1939-01-07\n",
      "3    735   DR-3  1930-01-12\n",
      "4    751   DR-3  1930-02-26\n",
      "5    752   DR-3         NaN\n",
      "6    837  MSK-4  1932-01-14\n",
      "7    844   DR-1  1932-03-22\n",
      "    taken person quant  reading\n",
      "0     619   dyer   rad     9.82\n",
      "1     619   dyer   sal     0.13\n",
      "2     622   dyer   rad     7.80\n",
      "3     622   dyer   sal     0.09\n",
      "4     734     pb   rad     8.41\n",
      "5     734   lake   sal     0.05\n",
      "6     734     pb  temp   -21.50\n",
      "7     735     pb   rad     7.22\n",
      "8     735    NaN   sal     0.06\n",
      "9     735    NaN  temp   -26.00\n",
      "10    751     pb   rad     4.35\n",
      "11    751     pb  temp   -18.50\n",
      "12    751   lake   sal     0.10\n",
      "13    752   lake   rad     2.19\n",
      "14    752   lake   sal     0.09\n",
      "15    752   lake  temp   -16.00\n",
      "16    752    roe   sal    41.60\n",
      "17    837   lake   rad     1.46\n",
      "18    837   lake   sal     0.21\n",
      "19    837    roe   sal    22.50\n",
      "20    844    roe   rad    11.25\n"
     ]
    }
   ],
   "source": [
    "#데이터 연결 마무리\n",
    "#판다스는 데이터 연결 전용 메서드인 merge를 제공한다. \n",
    "#다음은 특정 위치의 날씨 정보에 필요한 데이터 집합을 모두 불러온 것이다.\n",
    "#person은 관측한 사람의 이름, site는 관측 위치, visited는 관측 날짜,\n",
    "#survey는 날씨 정보이다. \n",
    "person = pd.read_csv('c:\\\\work\\\\survey_person.csv')\n",
    "site = pd.read_csv('c:\\\\work\\\\survey_site.csv')\n",
    "survey = pd.read_csv('c:\\\\work\\\\survey_survey.csv')\n",
    "visited = pd.read_csv('c:\\\\work\\\\survey_visited.csv')\n",
    "print(person)\n",
    "print(site)\n",
    "print(visited)\n",
    "print(survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visited데이터프레임의 일부 데이터만 떼어 실습에 사용한다.\n",
    "visited_subset = visited.loc[[0,2,6],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name    lat    long  ident   site       dated\n",
      "0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08\n",
      "1   DR-3 -47.15 -126.72    734   DR-3  1939-01-07\n",
      "2  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14\n"
     ]
    }
   ],
   "source": [
    "#merge메서드는 기본적으로 내부 조인을 실행하며 메서드를 사용한\n",
    "#데이터프레임(site)를 왼쪽으로 지정하고 첫번째 인자값으로 \n",
    "#지정한 데이터프레임(visited_subset)을 오른쪽으로 지정한다.\n",
    "#left_on, right_on인자는 값이 일치해야 할 왼쪽과 오른쪽 \n",
    "#데이터프레임의 열을 지정한다. 즉, 왼쪽 데이터프레임(site)의 열(name)과\n",
    "#오른쪽 데이터프레임(visited)의 열(site)의 값이 일치하면 \n",
    "#왼쪽 데이터프레임을 기준으로 연결한다. \n",
    "o2o_merge = site.merge(visited_subset, left_on='name', right_on='site')\n",
    "print(o2o_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name    lat    long  ident   site       dated\n",
      "0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08\n",
      "1   DR-1 -49.85 -128.57    622   DR-1  1927-02-10\n",
      "2   DR-1 -49.85 -128.57    844   DR-1  1932-03-22\n",
      "3   DR-3 -47.15 -126.72    734   DR-3  1939-01-07\n",
      "4   DR-3 -47.15 -126.72    735   DR-3  1930-01-12\n",
      "5   DR-3 -47.15 -126.72    751   DR-3  1930-02-26\n",
      "6   DR-3 -47.15 -126.72    752   DR-3         NaN\n",
      "7  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14\n"
     ]
    }
   ],
   "source": [
    "#다음은 site, visited데이터프레임을 이용하여 데이터를 연결한 것이다.\n",
    "m2o_merge = site.merge(visited, left_on='name', right_on='site')\n",
    "print(m2o_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ident   personal   family  taken person quant  reading\n",
      "0   dyer    William     Dyer    619   dyer   rad     9.82\n",
      "1   dyer    William     Dyer    619   dyer   sal     0.13\n",
      "2   dyer    William     Dyer    622   dyer   rad     7.80\n",
      "3   dyer    William     Dyer    622   dyer   sal     0.09\n",
      "4     pb      Frank  Pabodie    734     pb   rad     8.41\n",
      "5     pb      Frank  Pabodie    734     pb  temp   -21.50\n",
      "6     pb      Frank  Pabodie    735     pb   rad     7.22\n",
      "7     pb      Frank  Pabodie    751     pb   rad     4.35\n",
      "8     pb      Frank  Pabodie    751     pb  temp   -18.50\n",
      "9   lake   Anderson     Lake    734   lake   sal     0.05\n",
      "10  lake   Anderson     Lake    751   lake   sal     0.10\n",
      "11  lake   Anderson     Lake    752   lake   rad     2.19\n",
      "12  lake   Anderson     Lake    752   lake   sal     0.09\n",
      "13  lake   Anderson     Lake    752   lake  temp   -16.00\n",
      "14  lake   Anderson     Lake    837   lake   rad     1.46\n",
      "15  lake   Anderson     Lake    837   lake   sal     0.21\n",
      "16   roe  Valentina  Roerich    752    roe   sal    41.60\n",
      "17   roe  Valentina  Roerich    837    roe   sal    22.50\n",
      "18   roe  Valentina  Roerich    844    roe   rad    11.25\n"
     ]
    }
   ],
   "source": [
    "#다른 데이터프레임도 연결해 본다.\n",
    "#다음은 person, survey데이터프레임과 visited, \n",
    "#survey데이터프레임을 merge메서드로 연결한 것이다.\n",
    "ps = person.merge(survey, left_on='ident', right_on='person')\n",
    "vs = visited.merge(survey, left_on='ident', right_on='taken')\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ident   site       dated  taken person quant  reading\n",
      "0     619   DR-1  1927-02-08    619   dyer   rad     9.82\n",
      "1     619   DR-1  1927-02-08    619   dyer   sal     0.13\n",
      "2     622   DR-1  1927-02-10    622   dyer   rad     7.80\n",
      "3     622   DR-1  1927-02-10    622   dyer   sal     0.09\n",
      "4     734   DR-3  1939-01-07    734     pb   rad     8.41\n",
      "5     734   DR-3  1939-01-07    734   lake   sal     0.05\n",
      "6     734   DR-3  1939-01-07    734     pb  temp   -21.50\n",
      "7     735   DR-3  1930-01-12    735     pb   rad     7.22\n",
      "8     735   DR-3  1930-01-12    735    NaN   sal     0.06\n",
      "9     735   DR-3  1930-01-12    735    NaN  temp   -26.00\n",
      "10    751   DR-3  1930-02-26    751     pb   rad     4.35\n",
      "11    751   DR-3  1930-02-26    751     pb  temp   -18.50\n",
      "12    751   DR-3  1930-02-26    751   lake   sal     0.10\n",
      "13    752   DR-3         NaN    752   lake   rad     2.19\n",
      "14    752   DR-3         NaN    752   lake   sal     0.09\n",
      "15    752   DR-3         NaN    752   lake  temp   -16.00\n",
      "16    752   DR-3         NaN    752    roe   sal    41.60\n",
      "17    837  MSK-4  1932-01-14    837   lake   rad     1.46\n",
      "18    837  MSK-4  1932-01-14    837   lake   sal     0.21\n",
      "19    837  MSK-4  1932-01-14    837    roe   sal    22.50\n",
      "20    844   DR-1  1932-03-22    844    roe   rad    11.25\n"
     ]
    }
   ],
   "source": [
    "print(vs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ident_x           dyer\n",
      "personal       William\n",
      "family            Dyer\n",
      "taken_x            619\n",
      "person_x          dyer\n",
      "quant              rad\n",
      "reading           9.82\n",
      "ident_y            619\n",
      "site              DR-1\n",
      "dated       1927-02-08\n",
      "taken_y            619\n",
      "person_y          dyer\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#left_on, right_on에 전달하는 값은 여러 개라도 상관이 없다.\n",
    "#다음과 같이 여러 개의 열 이름을 리스트에 담아 전달해도 된다. \n",
    "#다음은 ps데이터프레임의 ident, taken, quant, reading열의 값과 \n",
    "#vs데이터프레임의 person, ident, quant, reading열의 값을 \n",
    "#이용하여 ps와 vs데이터프레임을 서로 연결한 것이다.\n",
    "ps_vs = ps.merge(vs, left_on=['ident','taken','quant','reading'],\n",
    "    right_on=['person','ident','quant','reading'])\n",
    "print(ps_vs.loc[0, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident_x</th>\n",
       "      <th>personal</th>\n",
       "      <th>family</th>\n",
       "      <th>taken_x</th>\n",
       "      <th>person_x</th>\n",
       "      <th>quant</th>\n",
       "      <th>reading</th>\n",
       "      <th>ident_y</th>\n",
       "      <th>site</th>\n",
       "      <th>dated</th>\n",
       "      <th>taken_y</th>\n",
       "      <th>person_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dyer</td>\n",
       "      <td>William</td>\n",
       "      <td>Dyer</td>\n",
       "      <td>619</td>\n",
       "      <td>dyer</td>\n",
       "      <td>rad</td>\n",
       "      <td>9.82</td>\n",
       "      <td>619</td>\n",
       "      <td>DR-1</td>\n",
       "      <td>1927-02-08</td>\n",
       "      <td>619</td>\n",
       "      <td>dyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dyer</td>\n",
       "      <td>William</td>\n",
       "      <td>Dyer</td>\n",
       "      <td>619</td>\n",
       "      <td>dyer</td>\n",
       "      <td>sal</td>\n",
       "      <td>0.13</td>\n",
       "      <td>619</td>\n",
       "      <td>DR-1</td>\n",
       "      <td>1927-02-08</td>\n",
       "      <td>619</td>\n",
       "      <td>dyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dyer</td>\n",
       "      <td>William</td>\n",
       "      <td>Dyer</td>\n",
       "      <td>622</td>\n",
       "      <td>dyer</td>\n",
       "      <td>rad</td>\n",
       "      <td>7.80</td>\n",
       "      <td>622</td>\n",
       "      <td>DR-1</td>\n",
       "      <td>1927-02-10</td>\n",
       "      <td>622</td>\n",
       "      <td>dyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dyer</td>\n",
       "      <td>William</td>\n",
       "      <td>Dyer</td>\n",
       "      <td>622</td>\n",
       "      <td>dyer</td>\n",
       "      <td>sal</td>\n",
       "      <td>0.09</td>\n",
       "      <td>622</td>\n",
       "      <td>DR-1</td>\n",
       "      <td>1927-02-10</td>\n",
       "      <td>622</td>\n",
       "      <td>dyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pb</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Pabodie</td>\n",
       "      <td>734</td>\n",
       "      <td>pb</td>\n",
       "      <td>rad</td>\n",
       "      <td>8.41</td>\n",
       "      <td>734</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>1939-01-07</td>\n",
       "      <td>734</td>\n",
       "      <td>pb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pb</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Pabodie</td>\n",
       "      <td>734</td>\n",
       "      <td>pb</td>\n",
       "      <td>temp</td>\n",
       "      <td>-21.50</td>\n",
       "      <td>734</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>1939-01-07</td>\n",
       "      <td>734</td>\n",
       "      <td>pb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pb</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Pabodie</td>\n",
       "      <td>735</td>\n",
       "      <td>pb</td>\n",
       "      <td>rad</td>\n",
       "      <td>7.22</td>\n",
       "      <td>735</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>1930-01-12</td>\n",
       "      <td>735</td>\n",
       "      <td>pb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pb</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Pabodie</td>\n",
       "      <td>751</td>\n",
       "      <td>pb</td>\n",
       "      <td>rad</td>\n",
       "      <td>4.35</td>\n",
       "      <td>751</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>1930-02-26</td>\n",
       "      <td>751</td>\n",
       "      <td>pb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pb</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Pabodie</td>\n",
       "      <td>751</td>\n",
       "      <td>pb</td>\n",
       "      <td>temp</td>\n",
       "      <td>-18.50</td>\n",
       "      <td>751</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>1930-02-26</td>\n",
       "      <td>751</td>\n",
       "      <td>pb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lake</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Lake</td>\n",
       "      <td>734</td>\n",
       "      <td>lake</td>\n",
       "      <td>sal</td>\n",
       "      <td>0.05</td>\n",
       "      <td>734</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>1939-01-07</td>\n",
       "      <td>734</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lake</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Lake</td>\n",
       "      <td>751</td>\n",
       "      <td>lake</td>\n",
       "      <td>sal</td>\n",
       "      <td>0.10</td>\n",
       "      <td>751</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>1930-02-26</td>\n",
       "      <td>751</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lake</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Lake</td>\n",
       "      <td>752</td>\n",
       "      <td>lake</td>\n",
       "      <td>rad</td>\n",
       "      <td>2.19</td>\n",
       "      <td>752</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lake</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Lake</td>\n",
       "      <td>752</td>\n",
       "      <td>lake</td>\n",
       "      <td>sal</td>\n",
       "      <td>0.09</td>\n",
       "      <td>752</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lake</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Lake</td>\n",
       "      <td>752</td>\n",
       "      <td>lake</td>\n",
       "      <td>temp</td>\n",
       "      <td>-16.00</td>\n",
       "      <td>752</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lake</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Lake</td>\n",
       "      <td>837</td>\n",
       "      <td>lake</td>\n",
       "      <td>rad</td>\n",
       "      <td>1.46</td>\n",
       "      <td>837</td>\n",
       "      <td>MSK-4</td>\n",
       "      <td>1932-01-14</td>\n",
       "      <td>837</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lake</td>\n",
       "      <td>Anderson</td>\n",
       "      <td>Lake</td>\n",
       "      <td>837</td>\n",
       "      <td>lake</td>\n",
       "      <td>sal</td>\n",
       "      <td>0.21</td>\n",
       "      <td>837</td>\n",
       "      <td>MSK-4</td>\n",
       "      <td>1932-01-14</td>\n",
       "      <td>837</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roe</td>\n",
       "      <td>Valentina</td>\n",
       "      <td>Roerich</td>\n",
       "      <td>752</td>\n",
       "      <td>roe</td>\n",
       "      <td>sal</td>\n",
       "      <td>41.60</td>\n",
       "      <td>752</td>\n",
       "      <td>DR-3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752</td>\n",
       "      <td>roe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roe</td>\n",
       "      <td>Valentina</td>\n",
       "      <td>Roerich</td>\n",
       "      <td>837</td>\n",
       "      <td>roe</td>\n",
       "      <td>sal</td>\n",
       "      <td>22.50</td>\n",
       "      <td>837</td>\n",
       "      <td>MSK-4</td>\n",
       "      <td>1932-01-14</td>\n",
       "      <td>837</td>\n",
       "      <td>roe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>roe</td>\n",
       "      <td>Valentina</td>\n",
       "      <td>Roerich</td>\n",
       "      <td>844</td>\n",
       "      <td>roe</td>\n",
       "      <td>rad</td>\n",
       "      <td>11.25</td>\n",
       "      <td>844</td>\n",
       "      <td>DR-1</td>\n",
       "      <td>1932-03-22</td>\n",
       "      <td>844</td>\n",
       "      <td>roe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ident_x   personal   family  taken_x person_x quant  reading  ident_y  \\\n",
       "0     dyer    William     Dyer      619     dyer   rad     9.82      619   \n",
       "1     dyer    William     Dyer      619     dyer   sal     0.13      619   \n",
       "2     dyer    William     Dyer      622     dyer   rad     7.80      622   \n",
       "3     dyer    William     Dyer      622     dyer   sal     0.09      622   \n",
       "4       pb      Frank  Pabodie      734       pb   rad     8.41      734   \n",
       "5       pb      Frank  Pabodie      734       pb  temp   -21.50      734   \n",
       "6       pb      Frank  Pabodie      735       pb   rad     7.22      735   \n",
       "7       pb      Frank  Pabodie      751       pb   rad     4.35      751   \n",
       "8       pb      Frank  Pabodie      751       pb  temp   -18.50      751   \n",
       "9     lake   Anderson     Lake      734     lake   sal     0.05      734   \n",
       "10    lake   Anderson     Lake      751     lake   sal     0.10      751   \n",
       "11    lake   Anderson     Lake      752     lake   rad     2.19      752   \n",
       "12    lake   Anderson     Lake      752     lake   sal     0.09      752   \n",
       "13    lake   Anderson     Lake      752     lake  temp   -16.00      752   \n",
       "14    lake   Anderson     Lake      837     lake   rad     1.46      837   \n",
       "15    lake   Anderson     Lake      837     lake   sal     0.21      837   \n",
       "16     roe  Valentina  Roerich      752      roe   sal    41.60      752   \n",
       "17     roe  Valentina  Roerich      837      roe   sal    22.50      837   \n",
       "18     roe  Valentina  Roerich      844      roe   rad    11.25      844   \n",
       "\n",
       "     site       dated  taken_y person_y  \n",
       "0    DR-1  1927-02-08      619     dyer  \n",
       "1    DR-1  1927-02-08      619     dyer  \n",
       "2    DR-1  1927-02-10      622     dyer  \n",
       "3    DR-1  1927-02-10      622     dyer  \n",
       "4    DR-3  1939-01-07      734       pb  \n",
       "5    DR-3  1939-01-07      734       pb  \n",
       "6    DR-3  1930-01-12      735       pb  \n",
       "7    DR-3  1930-02-26      751       pb  \n",
       "8    DR-3  1930-02-26      751       pb  \n",
       "9    DR-3  1939-01-07      734     lake  \n",
       "10   DR-3  1930-02-26      751     lake  \n",
       "11   DR-3         NaN      752     lake  \n",
       "12   DR-3         NaN      752     lake  \n",
       "13   DR-3         NaN      752     lake  \n",
       "14  MSK-4  1932-01-14      837     lake  \n",
       "15  MSK-4  1932-01-14      837     lake  \n",
       "16   DR-3         NaN      752      roe  \n",
       "17  MSK-4  1932-01-14      837      roe  \n",
       "18   DR-1  1932-03-22      844      roe  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6-1 누락값이란?\n",
    "#누락값은 NaN, NAN, nan과 같은 방법으로 표기한다. \n",
    "#먼저 누락값을 사용하기 위해 numpy에서 누락값을 불러온다.\n",
    "from numpy import NaN, NAN, nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#누락값은 0, ''와 같은 값과는 다른 개념이다. \n",
    "#누락값은 말 그대로 데이터 자체가 없다는 것을 의미한다. \n",
    "print(NaN == True)\n",
    "print(NaN == False)\n",
    "print(NaN == 0)\n",
    "print(NaN == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#누락값은 값 자체가 없기 때문에 \n",
    "#자기 자신과 비교해도 False가 출력된다.\n",
    "print(NaN == NaN)\n",
    "print(NaN == nan)\n",
    "print(NaN == NAN)\n",
    "print(nan == NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#그러면 누락값을 어떻게 확인할 수 있을까?\n",
    "#다행히 판다스에는 누락값을 확인하는 isnull메서드가 있다.\n",
    "import pandas as pd \n",
    "\n",
    "print(pd.isnull(NaN))\n",
    "print(pd.isnull(nan))\n",
    "print(pd.isnull(NAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#누락값이 아닌 경우?\n",
    "print(pd.notnull(NaN))\n",
    "print(pd.notnull(42))\n",
    "print(pd.notnull('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ident   site       dated\n",
      "0    619   DR-1  1927-02-08\n",
      "1    622   DR-1  1927-02-10\n",
      "2    734   DR-3  1939-01-07\n",
      "3    735   DR-3  1930-01-12\n",
      "4    751   DR-3  1930-02-26\n",
      "5    752   DR-3         NaN\n",
      "6    837  MSK-4  1932-01-14\n",
      "7    844   DR-1  1932-03-22\n",
      "    taken person quant  reading\n",
      "0     619   dyer   rad     9.82\n",
      "1     619   dyer   sal     0.13\n",
      "2     622   dyer   rad     7.80\n",
      "3     622   dyer   sal     0.09\n",
      "4     734     pb   rad     8.41\n",
      "5     734   lake   sal     0.05\n",
      "6     734     pb  temp   -21.50\n",
      "7     735     pb   rad     7.22\n",
      "8     735    NaN   sal     0.06\n",
      "9     735    NaN  temp   -26.00\n",
      "10    751     pb   rad     4.35\n",
      "11    751     pb  temp   -18.50\n",
      "12    751   lake   sal     0.10\n",
      "13    752   lake   rad     2.19\n",
      "14    752   lake   sal     0.09\n",
      "15    752   lake  temp   -16.00\n",
      "16    752    roe   sal    41.60\n",
      "17    837   lake   rad     1.46\n",
      "18    837   lake   sal     0.21\n",
      "19    837    roe   sal    22.50\n",
      "20    844    roe   rad    11.25\n"
     ]
    }
   ],
   "source": [
    "#누락값이 생기는 이유\n",
    "#누락값이 있는 데이터 집합을 연결할 때 누락값이 생기는 경우\n",
    "visited = pd.read_csv(\"c:\\\\work\\\\survey_visited.csv\")\n",
    "survey = pd.read_csv(\"c:\\\\work\\\\survey_survey.csv\")\n",
    "print(visited)\n",
    "print(survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ident   site       dated  taken person quant  reading\n",
      "0     619   DR-1  1927-02-08    619   dyer   rad     9.82\n",
      "1     619   DR-1  1927-02-08    619   dyer   sal     0.13\n",
      "2     622   DR-1  1927-02-10    622   dyer   rad     7.80\n",
      "3     622   DR-1  1927-02-10    622   dyer   sal     0.09\n",
      "4     734   DR-3  1939-01-07    734     pb   rad     8.41\n",
      "5     734   DR-3  1939-01-07    734   lake   sal     0.05\n",
      "6     734   DR-3  1939-01-07    734     pb  temp   -21.50\n",
      "7     735   DR-3  1930-01-12    735     pb   rad     7.22\n",
      "8     735   DR-3  1930-01-12    735    NaN   sal     0.06\n",
      "9     735   DR-3  1930-01-12    735    NaN  temp   -26.00\n",
      "10    751   DR-3  1930-02-26    751     pb   rad     4.35\n",
      "11    751   DR-3  1930-02-26    751     pb  temp   -18.50\n",
      "12    751   DR-3  1930-02-26    751   lake   sal     0.10\n",
      "13    752   DR-3         NaN    752   lake   rad     2.19\n",
      "14    752   DR-3         NaN    752   lake   sal     0.09\n",
      "15    752   DR-3         NaN    752   lake  temp   -16.00\n",
      "16    752   DR-3         NaN    752    roe   sal    41.60\n",
      "17    837  MSK-4  1932-01-14    837   lake   rad     1.46\n",
      "18    837  MSK-4  1932-01-14    837   lake   sal     0.21\n",
      "19    837  MSK-4  1932-01-14    837    roe   sal    22.50\n",
      "20    844   DR-1  1932-03-22    844    roe   rad    11.25\n"
     ]
    }
   ],
   "source": [
    "#앞에서 만든 데이터집합을 연결해 볼까요? \n",
    "#그러면 누락값이 많이 생겨난다. \n",
    "vs = visited.merge(survey, left_on='ident', right_on='taken')\n",
    "print(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goat      4.0\n",
      "amoeba    NaN\n",
      "dtype: float64\n",
      "<class 'pandas.core.series.Series'>\n",
      "                Name    Occupation        Born        Died  missing\n",
      "0  Rosaline Franklin       Chemist  1920-07-25  1958-04-16      NaN\n",
      "1     William Gosset  Statistician  1876-06-13  1937-10-16      NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#데이터를 입력할 때 누락값이 생기는 경우\n",
    "#누락값은 데이터를 잘못 입력하여 생길 수도 있다. \n",
    "#다음은 시리즈를 생성할 때 데이터프레임에 없는 열과 행 데이터를 \n",
    "#입력하여 누락값이 생긴 것이다. \n",
    "num_legs = pd.Series({'goat':4, 'amoeba':nan})\n",
    "print(num_legs)\n",
    "print(type(num_legs))\n",
    "\n",
    "scientists = pd.DataFrame({\n",
    "    'Name':['Rosaline Franklin', 'William Gosset'],\n",
    "    'Occupation':['Chemist', 'Statistician'],\n",
    "    'Born':['1920-07-25','1876-06-13'],\n",
    "    'Died':['1958-04-16','1937-10-16'],\n",
    "    'missing':[NaN,nan]\n",
    "})\n",
    "\n",
    "print(scientists)\n",
    "print(type(scientists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#범위를 지정하여 데이터를 추출할 때 누락값이 생기는 경우\n",
    "gapminder = pd.read_csv('c:\\\\work\\\\gapminder.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "1952    49.057620\n",
      "1957    51.507401\n",
      "1962    53.609249\n",
      "1967    55.678290\n",
      "1972    57.647386\n",
      "1977    59.570157\n",
      "1982    61.533197\n",
      "1987    63.212613\n",
      "1992    64.160338\n",
      "1997    65.014676\n",
      "2002    65.694923\n",
      "2007    67.007423\n",
      "Name: lifeExp, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#연도별로 그룹화한 다음 lifeExp열의 평균을 구한다.\n",
    "life_exp = gapminder.groupby(['year'])['lifeExp'].mean()\n",
    "print(life_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다음은 range메서드를 이용하여 life_Exp열에서 2000~2009년 데이터를 추출한다. \n",
    "#그런데 이렇게 데이터를 추출하면 없었던 연도가 포함되기 때문에 누락값이 많이 발생한다.\n",
    "#print(life_exp.loc[range(2000,2010), ])|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2002    65.694923\n",
      "2007    67.007423\n",
      "Name: lifeExp, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#앞에서 발생한 문제를 해결하기 위해서 불린 추출을 이용하여 \n",
    "#데이터를 추출하면 된다. \n",
    "y2000 = life_exp[life_exp.index > 2000]\n",
    "print(y2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#누락값의 개수\n",
    "ebola = pd.read_csv('c:\\\\work\\\\country_timeseries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                   122\n",
      "Day                    122\n",
      "Cases_Guinea            93\n",
      "Cases_Liberia           83\n",
      "Cases_SierraLeone       87\n",
      "Cases_Nigeria           38\n",
      "Cases_Senegal           25\n",
      "Cases_UnitedStates      18\n",
      "Cases_Spain             16\n",
      "Cases_Mali              12\n",
      "Deaths_Guinea           92\n",
      "Deaths_Liberia          81\n",
      "Deaths_SierraLeone      87\n",
      "Deaths_Nigeria          38\n",
      "Deaths_Senegal          22\n",
      "Deaths_UnitedStates     18\n",
      "Deaths_Spain            16\n",
      "Deaths_Mali             12\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#먼저 count메서드로 누락값이 아닌 개수를 구해본다. \n",
    "print(ebola.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                     0\n",
      "Day                      0\n",
      "Cases_Guinea            29\n",
      "Cases_Liberia           39\n",
      "Cases_SierraLeone       35\n",
      "Cases_Nigeria           84\n",
      "Cases_Senegal           97\n",
      "Cases_UnitedStates     104\n",
      "Cases_Spain            106\n",
      "Cases_Mali             110\n",
      "Deaths_Guinea           30\n",
      "Deaths_Liberia          41\n",
      "Deaths_SierraLeone      35\n",
      "Deaths_Nigeria          84\n",
      "Deaths_Senegal         100\n",
      "Deaths_UnitedStates    104\n",
      "Deaths_Spain           106\n",
      "Deaths_Mali            110\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#shape[0]에 전체 행의 데이터 개수가 저장되어 있다는 점을 \n",
    "#이용하여 shape[0]에서 누락값이 아닌 값의 개수를 빼면 \n",
    "#누락값의 개수를 구할 수 있다.\n",
    "num_rows = ebola.shape[0]\n",
    "num_missing = num_rows - ebola.count()\n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1214\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "#count_nonzero, isnull메서드를 조합해도 누락값의 개수를 구할 수 있다.\n",
    "#count_nonzero메서는 배열에서 0이 아닌 값의 개수를 세는 메서드이다.\n",
    "import numpy as np\n",
    "\n",
    "print(np.count_nonzero(ebola.isnull()))\n",
    "print(np.count_nonzero(ebola['Cases_Guinea'].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN      29\n",
      "86.0      3\n",
      "495.0     2\n",
      "112.0     2\n",
      "390.0     2\n",
      "Name: Cases_Guinea, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#시리즈에 포함된 value_counts메서드는 지정한 열의 빈도를 구하는 메서드이다.\n",
    "#value_counts메서드를 사용해 Cases_Guinea열의 누락값 개수를 구하려면 \n",
    "#다음과 같이 입력한다.\n",
    "print(ebola.Cases_Guinea.value_counts(dropna=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone\n",
      "0    1/5/2015  289        2776.0            0.0            10030.0\n",
      "1    1/4/2015  288        2775.0            0.0             9780.0\n",
      "2    1/3/2015  287        2769.0         8166.0             9722.0\n",
      "3    1/2/2015  286           0.0         8157.0                0.0\n",
      "4  12/31/2014  284        2730.0         8115.0             9633.0\n",
      "5  12/28/2014  281        2706.0         8018.0             9446.0\n",
      "6  12/27/2014  280        2695.0            0.0             9409.0\n",
      "7  12/24/2014  277        2630.0         7977.0             9203.0\n",
      "8  12/21/2014  273        2597.0            0.0             9004.0\n",
      "9  12/20/2014  272        2571.0         7862.0             8939.0\n"
     ]
    }
   ],
   "source": [
    "#누락값 처리하기 \n",
    "#데이터프레임에 포함된 fillna메서드에 0을 대입하면 누락값을 0으로 변경한다.\n",
    "#fillna메서드는 처리해야 하는 데이터프레임의 크기가 매우 크고 메모리를 \n",
    "#효율적으로 사용해야 하는 경우에 자주 사용하는 메서드이다.\n",
    "print(ebola.fillna(0).iloc[0:10, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone\n",
      "0    1/5/2015  289        2776.0            NaN            10030.0\n",
      "1    1/4/2015  288        2775.0            NaN             9780.0\n",
      "2    1/3/2015  287        2769.0         8166.0             9722.0\n",
      "3    1/2/2015  286        2769.0         8157.0             9722.0\n",
      "4  12/31/2014  284        2730.0         8115.0             9633.0\n",
      "5  12/28/2014  281        2706.0         8018.0             9446.0\n",
      "6  12/27/2014  280        2695.0         8018.0             9409.0\n",
      "7  12/24/2014  277        2630.0         7977.0             9203.0\n",
      "8  12/21/2014  273        2597.0         7977.0             9004.0\n",
      "9  12/20/2014  272        2571.0         7862.0             8939.0\n"
     ]
    }
   ],
   "source": [
    "#fillna메서드의 method인자값을 ffill로 지정하면 누락값이 나타나기 전의 값으로\n",
    "#누락값이 변경된다. 예를 들어 6행의 누락값은 누락값이 나타나기 전의 값인 5행의\n",
    "#값을 사용하여 누락값을 처리한다. 하지만 0,1행은 처음부터 누락값이기 때문에 \n",
    "#누락값이 그대로 남아 있다.\n",
    "print(ebola.fillna(method='ffill').iloc[0:10, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone\n",
      "0    1/5/2015  289        2776.0         8166.0            10030.0\n",
      "1    1/4/2015  288        2775.0         8166.0             9780.0\n",
      "2    1/3/2015  287        2769.0         8166.0             9722.0\n",
      "3    1/2/2015  286        2730.0         8157.0             9633.0\n",
      "4  12/31/2014  284        2730.0         8115.0             9633.0\n",
      "5  12/28/2014  281        2706.0         8018.0             9446.0\n",
      "6  12/27/2014  280        2695.0         7977.0             9409.0\n",
      "7  12/24/2014  277        2630.0         7977.0             9203.0\n",
      "8  12/21/2014  273        2597.0         7862.0             9004.0\n",
      "9  12/20/2014  272        2571.0         7862.0             8939.0\n"
     ]
    }
   ],
   "source": [
    "#method인자값을 bfil로 지정하면 누락값이 나타난 이후의 첫번째 값으로\n",
    "#앞쪽의 누락값이 모두 변경된다. 즉 앞의 과정의 반대 방향으로 누락값을 처리한다고\n",
    "#생각하면 된다. 하지만 이 방법도 마지막 값이 누락값인 경우에는 처리하지 \n",
    "#못한다는 단점이 있다.\n",
    "print(ebola.fillna(method='bfill').iloc[0:10, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone\n",
      "0    1/5/2015  289        2776.0            NaN            10030.0\n",
      "1    1/4/2015  288        2775.0            NaN             9780.0\n",
      "2    1/3/2015  287        2769.0         8166.0             9722.0\n",
      "3    1/2/2015  286        2749.5         8157.0             9677.5\n",
      "4  12/31/2014  284        2730.0         8115.0             9633.0\n",
      "5  12/28/2014  281        2706.0         8018.0             9446.0\n",
      "6  12/27/2014  280        2695.0         7997.5             9409.0\n",
      "7  12/24/2014  277        2630.0         7977.0             9203.0\n",
      "8  12/21/2014  273        2597.0         7919.5             9004.0\n",
      "9  12/20/2014  272        2571.0         7862.0             8939.0\n"
     ]
    }
   ],
   "source": [
    "#interpolate메서드는 누락값 양쪽에 있는 값을 이용하여 \n",
    "#중간값을 구한 다음 누락값을 처리한다. 이렇게 하면 데이터프레임이\n",
    "#일정한 간격을 유지하고 있는 것처럼 수정할 수 있다.\n",
    "print(ebola.interpolate().iloc[0:10,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 18)\n"
     ]
    }
   ],
   "source": [
    "#누락값 삭제하기\n",
    "#누락값이 필요 없을 경우에는 누락값을 삭제해도 된다. \n",
    "print(ebola.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 18)\n"
     ]
    }
   ],
   "source": [
    "#누락값을 삭제하기 위해 dropna메서드를 사용한다. \n",
    "#누락값이 포함된 행이 모두 삭제되기 때문에 많은 데이터가 삭제된다.\n",
    "ebola_dropna = ebola.dropna()\n",
    "print(ebola_dropna.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Day  Cases_Guinea  Cases_Liberia  Cases_SierraLeone  \\\n",
      "19  11/18/2014  241        2047.0         7082.0             6190.0   \n",
      "\n",
      "    Cases_Nigeria  Cases_Senegal  Cases_UnitedStates  Cases_Spain  Cases_Mali  \\\n",
      "19           20.0            1.0                 4.0          1.0         6.0   \n",
      "\n",
      "    Deaths_Guinea  Deaths_Liberia  Deaths_SierraLeone  Deaths_Nigeria  \\\n",
      "19         1214.0          2963.0              1267.0             8.0   \n",
      "\n",
      "    Deaths_Senegal  Deaths_UnitedStates  Deaths_Spain  Deaths_Mali  \n",
      "19             0.0                  1.0           0.0          6.0  \n"
     ]
    }
   ],
   "source": [
    "print(ebola_dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#누락값이 포함된 데이터 계산하기 \n",
    "#Guinea, Liberia, SierraLeone열에는 누락값이 존재한다 \n",
    "#만약 누락값이 존재하는 Guinea, Liberia, SierraLeone열을 가지고 \n",
    "#ebola발병 수의 합을 계산하면 어떻게 될까? \n",
    "#다음은 Cases_Guinea, Cases_Liberia, Cases_SierraLeone열을 더하여\n",
    "#Cases_multiple열을 새로 만든 것이다.\n",
    "ebola['Cases_multiple'] = ebola['Cases_Guinea'] + ebola['Cases_Liberia'] + \\\n",
    "ebola['Cases_SierraLeone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Cases_Guinea  Cases_Liberia  Cases_SierraLeone  Cases_multiple\n",
      "0        2776.0            NaN            10030.0             NaN\n",
      "1        2775.0            NaN             9780.0             NaN\n",
      "2        2769.0         8166.0             9722.0         20657.0\n",
      "3           NaN         8157.0                NaN             NaN\n",
      "4        2730.0         8115.0             9633.0         20478.0\n",
      "5        2706.0         8018.0             9446.0         20170.0\n",
      "6        2695.0            NaN             9409.0             NaN\n",
      "7        2630.0         7977.0             9203.0         19810.0\n",
      "8        2597.0            NaN             9004.0             NaN\n",
      "9        2571.0         7862.0             8939.0         19372.0\n"
     ]
    }
   ],
   "source": [
    "#누락값이 하나라도 있는 행은 계산결과(Cases_multiple)가\n",
    "#NaN이 되었음을 알 수 있다.\n",
    "ebola_subset = ebola.loc[:, ['Cases_Guinea', 'Cases_Liberia', \\\n",
    "    'Cases_SierraLeone', 'Cases_multiple']]\n",
    "print(ebola_subset.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84729.0\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "#Cases_multiple열을 sum메서드를 사용해 더하면 세 지역의 \n",
    "#ebola발병 수의 합을 구할 수 있다. \n",
    "#이때 sum메서드를 그냥 하용하면 누락값을 포함해 계산한다. \n",
    "#따라서 결과값도 누락값이 된다. 누락값을 무시한 채 계산하려면\n",
    "#skipna인자값을 True로 설정하면 된다.\n",
    "print(ebola.Cases_Guinea.sum(skipna=True))\n",
    "print(ebola.Cases_Guinea.sum(skipna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion  <$10k  $10-20k  $20-30k  $30-40k  $40-50k  $50-75k  \\\n",
      "0            Agnostic     27       34       60       81       76      137   \n",
      "1             Atheist     12       27       37       52       35       70   \n",
      "2            Buddhist     27       21       30       34       33       58   \n",
      "3            Catholic    418      617      732      670      638     1116   \n",
      "4  Don’t know/refused     15       14       15       11       10       35   \n",
      "\n",
      "   $75-100k  $100-150k  >150k  Don't know/refused  \n",
      "0       122        109     84                  96  \n",
      "1        73         59     74                  76  \n",
      "2        62         39     53                  54  \n",
      "3       949        792    633                1489  \n",
      "4        21         17     18                 116  \n"
     ]
    }
   ],
   "source": [
    "#melt메서드 사용하기\n",
    "#1개의 열만 고정하고 나머지 열을 행으로 바꾸기\n",
    "#이번에 사용할 데이터집합은 퓨 리서치 센터에서 조사한 \n",
    "#'무국의 소득과 종교'라는 데이터이다. \n",
    "#이 데이터프레임에는 11개의 열이 있다. \n",
    "import pandas as pd \n",
    "pew = pd.read_csv('c:\\\\work\\\\pew.csv')\n",
    "print(pew.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   religion  <$10k  $10-20k  $20-30k  $30-40k  $40-50k\n",
      "0                  Agnostic     27       34       60       81       76\n",
      "1                   Atheist     12       27       37       52       35\n",
      "2                  Buddhist     27       21       30       34       33\n",
      "3                  Catholic    418      617      732      670      638\n",
      "4        Don’t know/refused     15       14       15       11       10\n",
      "5          Evangelical Prot    575      869     1064      982      881\n",
      "6                     Hindu      1        9        7        9       11\n",
      "7   Historically Black Prot    228      244      236      238      197\n",
      "8         Jehovah's Witness     20       27       24       24       21\n",
      "9                    Jewish     19       19       25       25       30\n",
      "10            Mainline Prot    289      495      619      655      651\n",
      "11                   Mormon     29       40       48       51       56\n",
      "12                   Muslim      6        7        9       10        9\n",
      "13                 Orthodox     13       17       23       32       32\n",
      "14          Other Christian      9        7       11       13       13\n",
      "15             Other Faiths     20       33       40       46       49\n",
      "16    Other World Religions      5        2        3        4        2\n",
      "17             Unaffiliated    217      299      374      365      341\n"
     ]
    }
   ],
   "source": [
    "#6개의 열만 출력해 본다. 그러면 종교와 소득 정보가 출력된다. \n",
    "#하지 이 상태는 소득 정보($10K,$10-20K...)가 열을 구성하고 있따. \n",
    "#만약 소득 정보 열을 행 데이터로 옮기고 싶다면 어떻게 해야 하나? \n",
    "print(pew.iloc[:, 0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion variable  value\n",
      "0            Agnostic    <$10k     27\n",
      "1             Atheist    <$10k     12\n",
      "2            Buddhist    <$10k     27\n",
      "3            Catholic    <$10k    418\n",
      "4  Don’t know/refused    <$10k     15\n"
     ]
    }
   ],
   "source": [
    "#id_vars인자값으로 지정한 열(religion)을 제외한 나머지 소득 정보열\n",
    "#($10k,$10-20K...)이 variable열로 정리되고 소득 정도 열의 행 데이터도\n",
    "#value열로 정리되었따. 바로 이 과정을 'religion'열을 고정하여 피벗했다'\n",
    "#라고 말한다. \n",
    "pew_long = pd.melt(pew, id_vars='religion')\n",
    "print(pew_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion income  count\n",
      "0            Agnostic  <$10k     27\n",
      "1             Atheist  <$10k     12\n",
      "2            Buddhist  <$10k     27\n",
      "3            Catholic  <$10k    418\n",
      "4  Don’t know/refused  <$10k     15\n"
     ]
    }
   ],
   "source": [
    "#그러면 variable, value열의 열 이름은 어떻게 바꿀 수 있나?\n",
    "#var_name, value_name인자값을 사용하면 된다. \n",
    "#다음은 variable, value라는 열 이름을 income, count로 재설정한 코드이다.\n",
    "pew_long = pd.melt(pew, id_vars='religion', var_name='income',\n",
    "    value_name='count')\n",
    "print(pew_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered  wk1   wk2  \\\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   87  82.0   \n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02   91  87.0   \n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08   81  70.0   \n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21   76  76.0   \n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15   57  34.0   \n",
      "\n",
      "    wk3   wk4   wk5   wk6   wk7   wk8   wk9  wk10  wk11  \n",
      "0  72.0  77.0  87.0  94.0  99.0   NaN   NaN   NaN   NaN  \n",
      "1  92.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2  68.0  67.0  66.0  57.0  54.0  53.0  51.0  51.0  51.0  \n",
      "3  72.0  69.0  67.0  65.0  55.0  59.0  62.0  61.0  61.0  \n",
      "4  25.0  17.0  17.0  31.0  36.0  49.0  53.0  57.0  64.0  \n"
     ]
    }
   ],
   "source": [
    "#2개 이상의 열을 고정하고 나머지 열을 행으로 바꾸기 \n",
    "#다음은 빌보드 차트의 데이터 집합을 불러온 다음 데잍를 확인한 것이다. \n",
    "billboard = pd.read_csv('c:\\\\work\\\\billboard.csv')\n",
    "print(billboard.iloc[0:5, 0:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered week  rating\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02  wk1    91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08  wk1    81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21  wk1    76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15  wk1    57.0\n"
     ]
    }
   ],
   "source": [
    "#다음은 year, artist, track, time, date.entered열을 모두 고정하고\n",
    "#나머지 열(wk1, wk1...)을 피벗한 것이다.\n",
    "billboard_long = pd.melt(billboard, id_vars=['year','artist',\n",
    "    'track','time','date.entered'], var_name='week', value_name='rating')\n",
    "print(billboard_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Day', 'Cases_Guinea', 'Cases_Liberia', 'Cases_SierraLeone',\n",
      "       'Cases_Nigeria', 'Cases_Senegal', 'Cases_UnitedStates', 'Cases_Spain',\n",
      "       'Cases_Mali', 'Deaths_Guinea', 'Deaths_Liberia', 'Deaths_SierraLeone',\n",
      "       'Deaths_Nigeria', 'Deaths_Senegal', 'Deaths_UnitedStates',\n",
      "       'Deaths_Spain', 'Deaths_Mali'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#7-2 열 이름 관리하기 \n",
    "#어떤 열은 여러 가지 의미를 가지고 있을 수 있따. \n",
    "#예를 들어 ebola데이터 집합의 열중 하나인 Deaths_Guinea는 \n",
    "#'사명자 수'와 '나라 이름'을 합쳐 만든 이름이다. \n",
    "#다음은 ebola데이터를 불러온 다음 0,1,2,3,10,11열의 5개 데이터만 \n",
    "#확인한 것이다.\n",
    "ebola = pd.read_csv('c:\\\\work\\\\country_timeseries.csv')\n",
    "print(ebola.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Deaths_Guinea  Deaths_Liberia\n",
      "0    1/5/2015  289        2776.0            NaN         1786.0             NaN\n",
      "1    1/4/2015  288        2775.0            NaN         1781.0             NaN\n",
      "2    1/3/2015  287        2769.0         8166.0         1767.0          3496.0\n",
      "3    1/2/2015  286           NaN         8157.0            NaN          3496.0\n",
      "4  12/31/2014  284        2730.0         8115.0         1739.0          3471.0\n"
     ]
    }
   ],
   "source": [
    "print(ebola.iloc[:5, [0,1,2,3,10,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0\n",
      "3    1/2/2015  286  Cases_Guinea     NaN\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0\n"
     ]
    }
   ],
   "source": [
    "#일단 Date와 Day를 고정하고 나머지를 행으로 피벗한다. \n",
    "#그러면 각 나라별 사망자 수를 행으로 볼 수 있어 편리하다. \n",
    "#열 이름(Cases_Guinea,...)을 분리하는 바로 다음 실습에서 진행한다.\n",
    "ebola_long = pd.melt(ebola, id_vars=['Date','Day'])\n",
    "print(ebola_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Cases, Guinea]\n",
      "1    [Cases, Guinea]\n",
      "2    [Cases, Guinea]\n",
      "3    [Cases, Guinea]\n",
      "4    [Cases, Guinea]\n",
      "Name: variable, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#split메서드로 열 이름 분리하기\n",
    "#Cases_Guinea와 같이 2개 이상의 의미를 가지고 있는 열 이름은 \n",
    "#밑줄을 기준으로 Cases, Guinea와 같은 방법으로 분리할 수 있따. \n",
    "variable_split = ebola_long.variable.str.split(\"_\")\n",
    "print(variable_split[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#이때 variable_split에 저장된 값의 자료형은 시리즈이고 각각의 시리즈에 \n",
    "#저장된 값(열이름)의 자료형은 리스트이다.\n",
    "print(type(variable_split))\n",
    "print(type(variable_split[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Cases\n",
      "1    Cases\n",
      "2    Cases\n",
      "3    Cases\n",
      "4    Cases\n",
      "Name: variable, dtype: object\n",
      "1947    Deaths\n",
      "1948    Deaths\n",
      "1949    Deaths\n",
      "1950    Deaths\n",
      "1951    Deaths\n",
      "Name: variable, dtype: object\n",
      "0    Guinea\n",
      "1    Guinea\n",
      "2    Guinea\n",
      "3    Guinea\n",
      "4    Guinea\n",
      "Name: variable, dtype: object\n",
      "1947    Mali\n",
      "1948    Mali\n",
      "1949    Mali\n",
      "1950    Mali\n",
      "1951    Mali\n",
      "Name: variable, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#이때 앞의 과정에서 구한 리스트의 0번째 인덱스에 담긴 문자열은 발병(Cases)\n",
    "#과 죽음(Deaths)같은 상태를 의미하고 1번째 인덱스에 담긴 문자열은 나라 이름을\n",
    "#의미한다. 이제 이 문자열을 분리하여 데이터프레임의 새로운 열로 추가하면 된다.\n",
    "status_values = variable_split.str.get(0)\n",
    "country_values = variable_split.str.get(1)\n",
    "print(status_values[:5])\n",
    "print(status_values[-5:])\n",
    "print(country_values[:5])\n",
    "print(country_values[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value status country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "#앞에서 분리한 문자열을 status, country라는 열 이름으로\n",
    "#데이터프레임에 추가한 코드이다.\n",
    "ebola_long['status'] = status_values\n",
    "ebola_long['country'] = country_values\n",
    "print(ebola_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value status country status country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "#concat메서드로 데이터프레임에 열 추가하기\n",
    "#concat메서드를 화룡ㅇ하면 split메서드로 분리한 데이터를 \n",
    "#바로 데이터프레임에 추가할 수 있다. \n",
    "#다음은 바로 앞에서 실습한 내용을 concat메서드로 바꿔 실행한 것이다.\n",
    "variable_split = ebola_long.variable.str.split('_', expand=True)\n",
    "variable_split.columns = ['status', 'country']\n",
    "ebola_parsed = pd.concat([ebola_long, variable_split], axis=1)\n",
    "\n",
    "print(ebola_parsed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  year  month element  d1    d2    d3  d4    d5  d6  ...  d22   d23  \\\n",
      "0  MX17004  2010      1    tmax NaN   NaN   NaN NaN   NaN NaN  ...  NaN   NaN   \n",
      "1  MX17004  2010      1    tmin NaN   NaN   NaN NaN   NaN NaN  ...  NaN   NaN   \n",
      "2  MX17004  2010      2    tmax NaN  27.3  24.1 NaN   NaN NaN  ...  NaN  29.9   \n",
      "3  MX17004  2010      2    tmin NaN  14.4  14.4 NaN   NaN NaN  ...  NaN  10.7   \n",
      "4  MX17004  2010      3    tmax NaN   NaN   NaN NaN  32.1 NaN  ...  NaN   NaN   \n",
      "\n",
      "   d24  d25  d26  d27  d28  d29   d30  d31  \n",
      "0  NaN  NaN  NaN  NaN  NaN  NaN  27.8  NaN  \n",
      "1  NaN  NaN  NaN  NaN  NaN  NaN  14.5  NaN  \n",
      "2  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "3  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "4  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "#7-3 여러 열을 하나로 정리하기\n",
    "#보통 데이터프레임의 열은 파이썬의 변수와 같은 개념으로 사용한다.\n",
    "#기상 데이터의 여러 열을 하나로 정리하기 - melt, pivot_table메서드\n",
    "#다음은 기상 데이터를 불러와 출력한 것이다. 날짜 열(d1,...d31)에는\n",
    "#각 월별 최고, 최저 온도 데이터가 저장되어 있따. \n",
    "#지금은 날짜 열이 옆으로 길게 늘어져 있어 보기 불편하다. \n",
    "weather = pd.read_csv(\"c:\\\\work\\\\weather.csv\")\n",
    "print(weather.iloc[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  year  month element day  temp\n",
      "0  MX17004  2010      1    tmax  d1   NaN\n",
      "1  MX17004  2010      1    tmin  d1   NaN\n",
      "2  MX17004  2010      2    tmax  d1   NaN\n",
      "3  MX17004  2010      2    tmin  d1   NaN\n",
      "4  MX17004  2010      3    tmax  d1   NaN\n"
     ]
    }
   ],
   "source": [
    "#다음은 melt메서드로 일별 온도 축정값(d1,d2,...)을 피벗한 것이다.\n",
    "#그러면 day열에 날짜 열이 정리되고 날짜 열의 데이터는 temp열에 정리된다. \n",
    "weather_melt = pd.melt(weather, id_vars=['id','year','month','element'],\n",
    "    var_name='day', value_name='temp')\n",
    "print(weather_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element                 tmax  tmin\n",
      "id      year month day            \n",
      "MX17004 2010 1     d30  27.8  14.5\n",
      "             2     d11  29.7  13.4\n",
      "                   d2   27.3  14.4\n",
      "                   d23  29.9  10.7\n",
      "                   d3   24.1  14.4\n",
      "             3     d10  34.5  16.8\n",
      "                   d16  31.1  17.6\n",
      "                   d5   32.1  14.2\n",
      "             4     d27  36.3  16.7\n",
      "             5     d27  33.2  18.2\n",
      "             6     d17  28.0  17.5\n",
      "                   d29  30.1  18.0\n",
      "             7     d3   28.6  17.5\n",
      "                   d14  29.9  16.5\n",
      "             8     d23  26.4  15.0\n",
      "                   d5   29.6  15.8\n",
      "                   d29  28.0  15.3\n",
      "                   d13  29.8  16.5\n",
      "                   d25  29.7  15.6\n",
      "                   d31  25.4  15.4\n",
      "                   d8   29.0  17.3\n",
      "             10    d5   27.0  14.0\n",
      "                   d14  29.5  13.0\n",
      "                   d15  28.7  10.5\n",
      "                   d28  31.2  15.0\n",
      "                   d7   28.1  12.9\n",
      "             11    d2   31.3  16.3\n",
      "                   d5   26.3   7.9\n",
      "                   d27  27.7  14.2\n",
      "                   d26  28.1  12.1\n",
      "                   d4   27.2  12.0\n",
      "             12    d1   29.9  13.8\n",
      "                   d6   27.8  10.5\n"
     ]
    }
   ],
   "source": [
    "#이제 pivot_table메서드를 사용할 차례이다.\n",
    "#pivot_table메서드는 행과 열의 위치를 다시 바꿔 정리해 준다. \n",
    "#index인자에는 위치를 그대로 유지할 열 이름을 지정하고,\n",
    "#columns인자에는 피벗할 열 이름을 지정하고, values인자에는 새로운 열의\n",
    "#데이터가 될 열 이름을 지정하면 된다. \n",
    "#다음은 pivot_table메서드로 행과 열의 위치를 다시 바꾼 것이다. \n",
    "weather_tidy = weather_melt.pivot_table(\n",
    "    index=['id','year','month','day'],\n",
    "    columns='element',\n",
    "    values='temp'\n",
    ")\n",
    "print(weather_tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element       id  year  month  day  tmax  tmin\n",
      "0        MX17004  2010      1  d30  27.8  14.5\n",
      "1        MX17004  2010      2  d11  29.7  13.4\n",
      "2        MX17004  2010      2   d2  27.3  14.4\n",
      "3        MX17004  2010      2  d23  29.9  10.7\n",
      "4        MX17004  2010      2   d3  24.1  14.4\n"
     ]
    }
   ],
   "source": [
    "#앞에서 구한 데이터프레임의 인덱스를 reset_index메서드로 \n",
    "#새로 지정한 것이다.\n",
    "weather_tidy_flat = weather_tidy.reset_index()\n",
    "print(weather_tidy_flat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 7)\n"
     ]
    }
   ],
   "source": [
    "#7-4 중복 데이터 처리하기 \n",
    "#다음은 빌보드 차트 데이터를 불러온 것이다.\n",
    "billboard = pd.read_csv('c:\\\\work\\\\billboard.csv')\n",
    "billboard_long = pd.melt(billboard, id_vars=['year','artist','track',\n",
    "    'time','date.entered'], var_name='week', value_name='rating')\n",
    "\n",
    "print(billboard_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered week  rating\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02  wk1    91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08  wk1    81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21  wk1    76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15  wk1    57.0\n"
     ]
    }
   ],
   "source": [
    "print(billboard_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year        artist  track  time date.entered week  rating\n",
      "3     2000  3 Doors Down  Loser  4:24   2000-10-21  wk1    76.0\n",
      "320   2000  3 Doors Down  Loser  4:24   2000-10-21  wk2    76.0\n",
      "637   2000  3 Doors Down  Loser  4:24   2000-10-21  wk3    72.0\n",
      "954   2000  3 Doors Down  Loser  4:24   2000-10-21  wk4    69.0\n",
      "1271  2000  3 Doors Down  Loser  4:24   2000-10-21  wk5    67.0\n"
     ]
    }
   ],
   "source": [
    "#노래 제목이 Loser인 데이터만 따로 모아 살펴보면 중복 데이터가\n",
    "#꽤 많다는 것을 알 수 있다. 예를 들어 가수(artist)는 고유한 값이기 \n",
    "#때문에 따로 관리하는 것이 데이터의 일관성을 유지하는데 도움이 된다. \n",
    "print(billboard_long[billboard_long.track == 'Loser'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 4)\n"
     ]
    }
   ],
   "source": [
    "#중복 데이터를 가지고 있는 열은 year, artist, track, time, date이다. \n",
    "#이 열을 따로 모아 새로운 데이터프레임에 저장한다.\n",
    "billboard_songs = billboard_long[['year','artist','track','time']]\n",
    "print(billboard_songs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317, 4)\n"
     ]
    }
   ],
   "source": [
    "#그런 다음 drop_duplicats메서드로 데이터프레임의 중복 데이터를 제거한다\n",
    "billboard_songs = billboard_songs.drop_duplicates()\n",
    "print(billboard_songs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year          artist                    track  time  id\n",
      "0  2000           2 Pac  Baby Don't Cry (Keep...  4:22   0\n",
      "1  2000         2Ge+her  The Hardest Part Of ...  3:15   1\n",
      "2  2000    3 Doors Down               Kryptonite  3:53   2\n",
      "3  2000    3 Doors Down                    Loser  4:24   3\n",
      "4  2000        504 Boyz            Wobble Wobble  3:35   4\n",
      "5  2000            98^0  Give Me Just One Nig...  3:24   5\n",
      "6  2000         A*Teens            Dancing Queen  3:44   6\n",
      "7  2000         Aaliyah            I Don't Wanna  4:15   7\n",
      "8  2000         Aaliyah                Try Again  4:03   8\n",
      "9  2000  Adams, Yolanda            Open My Heart  5:30   9\n"
     ]
    }
   ],
   "source": [
    "#중복을 제거한 데이터프레임에 다음과 같이 아이디(id)도 추가한다.\n",
    "billboard_songs['id'] = range(len(billboard_songs))\n",
    "print(billboard_songs.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 8)\n"
     ]
    }
   ],
   "source": [
    "#다음은 merge메서드를 사용해 노래 정보와 주간 순위 데이터를 합친 것이다.\n",
    "billboard_ratings = billboard_long.merge(billboard_songs, \n",
    "    on=['year','artist','track','time'])\n",
    "print(billboard_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year artist                    track  time date.entered week  rating  id\n",
      "0  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0   0\n",
      "1  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk2    82.0   0\n",
      "2  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk3    72.0   0\n",
      "3  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk4    77.0   0\n",
      "4  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk5    87.0   0\n"
     ]
    }
   ],
   "source": [
    "print(billboard_ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-01.csv\n",
      "\n",
      "c:\\work\\fhv_tripdata_2015-01.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-02.csv\n",
      "\n",
      "c:\\work\\fhv_tripdata_2015-02.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-03.csv\n",
      "\n",
      "c:\\work\\fhv_tripdata_2015-03.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-04.csv\n",
      "\n",
      "c:\\work\\fhv_tripdata_2015-04.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-05.csv\n",
      "\n",
      "c:\\work\\fhv_tripdata_2015-05.csv\n"
     ]
    }
   ],
   "source": [
    "#7-5 대용량 데이터 처리하기 \n",
    "#뉴욕 택시 데이터 준비하기 \n",
    "#뉴욕 택스 데이터는 13억 대의 뉴욕 택시에 대한 정보를 가지고 있다. \n",
    "#파일의 수도 140개나 된다.\n",
    "import os \n",
    "import urllib.request\n",
    "\n",
    "with open(\"c:\\\\work\\\\raw_data_urls.txt\", \"r\") as data_urls:\n",
    "    for line, url in enumerate(data_urls):\n",
    "        if line == 5:\n",
    "            break \n",
    "        fn = url.split('/')[-1].strip()\n",
    "        fp = os.path.join('', 'c:\\\\work\\\\', fn)\n",
    "        print(url)\n",
    "        print(fp)\n",
    "        urllib.request.urlretrieve(url, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\work\\\\fhv_tripdata_2015-01.csv', 'c:\\\\work\\\\fhv_tripdata_2015-02.csv', 'c:\\\\work\\\\fhv_tripdata_2015-03.csv', 'c:\\\\work\\\\fhv_tripdata_2015-04.csv', 'c:\\\\work\\\\fhv_tripdata_2015-05.csv']\n"
     ]
    }
   ],
   "source": [
    "#앞에서 만든 5개의 파일을 불러온다. \n",
    "import glob \n",
    "nyc_taxi_data = glob.glob(\"c:\\\\work\\\\fhv_*\")\n",
    "print(nyc_taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi1 = pd.read_csv(nyc_taxi_data[0])\n",
    "taxi2 = pd.read_csv(nyc_taxi_data[1])\n",
    "taxi3 = pd.read_csv(nyc_taxi_data[2])\n",
    "taxi4 = pd.read_csv(nyc_taxi_data[3])\n",
    "taxi5 = pd.read_csv(nyc_taxi_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-01-01 00:30:00         NaN\n",
      "1               B00013  2015-01-01 01:22:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-02-01 00:00:00         NaN\n",
      "1               B00013  2015-02-01 00:01:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00029  2015-03-01 00:02:00       213.0\n",
      "1               B00029  2015-03-01 00:03:00        51.0\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00001  2015-04-01 04:30:00         NaN\n",
      "1               B00001  2015-04-01 06:00:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00001  2015-05-01 04:30:00         NaN\n",
      "1               B00001  2015-05-01 05:00:00         NaN\n"
     ]
    }
   ],
   "source": [
    "print(taxi1.head(n=2))\n",
    "print(taxi2.head(n=2))\n",
    "print(taxi3.head(n=2))\n",
    "print(taxi4.head(n=2))\n",
    "print(taxi5.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2746033, 3)\n",
      "(3126401, 3)\n",
      "(3281427, 3)\n",
      "(3917789, 3)\n",
      "(4296067, 3)\n"
     ]
    }
   ],
   "source": [
    "print(taxi1.shape)\n",
    "print(taxi2.shape)\n",
    "print(taxi3.shape)\n",
    "print(taxi4.shape)\n",
    "print(taxi5.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17367717, 3)\n"
     ]
    }
   ],
   "source": [
    "#이제 데이터처리를 위해 각 데이터프레임을 연결한다. \n",
    "taxi = pd.concat([taxi1,taxi2,taxi3,taxi4,taxi5])\n",
    "print(taxi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-01-01 00:30:00         NaN\n",
      "1               B00013  2015-01-01 01:22:00         NaN\n",
      "2               B00013  2015-01-01 01:23:00         NaN\n",
      "3               B00013  2015-01-01 01:44:00         NaN\n",
      "4               B00013  2015-01-01 02:00:00         NaN\n",
      "(17367717, 3)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#반복문으로 동일한 데이터를 얻어오기\n",
    "list_taxi_df = []\n",
    "\n",
    "for csv_filename in nyc_taxi_data:\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    list_taxi_df.append(df)\n",
    "\n",
    "print(len(list_taxi_df))\n",
    "\n",
    "print(type(list_taxi_df[0]))\n",
    "\n",
    "print(list_taxi_df[0].head())\n",
    "\n",
    "taxi_loop_concat = pd.concat(list_taxi_df)\n",
    "print(taxi_loop_concat.shape)\n",
    "\n",
    "print(taxi.equals(taxi_loop_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
